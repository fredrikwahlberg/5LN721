{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tesseract example code\n",
    "\n",
    "To be able to use this code, you will need to have the packages opencv (image reading and maipluation), tesseract (OCR) and pytesseract (tesseract python bindings) installed. You can install pytesseract by running the following command in your terminal:\n",
    "\n",
    "```\n",
    "pip3 install --user pytesseract\n",
    "```\n",
    "\n",
    "More information can be found at: https://pypi.org/project/pytesseract/\n",
    "\n",
    "## Importing some modules\n",
    "\n",
    "We start with importing som modules that will come in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2                      # Computer Vision\n",
    "import numpy as np              # Vector math\n",
    "import pytesseract              # OCR\n",
    "import matplotlib.pyplot as plt # Plotting, the line after this one is only for jupyter notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading text from a file and render it to an image\n",
    "\n",
    "For testing tesseract it can be useful to be able to create an image with text and then convert the image back to text data. We can conceptualise this as a function $f: T \\rightarrow I$ from the text domain to the image domain. This will be implemented using [OpenCV](https://opencv.org/). We then define a function $g: I \\rightarrow T$, as an approximation of the inverse of $f$. Tesseract will stand in for the inverse function. After these steps, we can see if we got back what we started with. The character (or word) error rate can be measured as a way to quantify the quality of $g$. For testing the robustness of $g$, we will insert some image noise between $f$ and $g$. This is the qualitative part of the lab.\n",
    "\n",
    "First, we'll need to read some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in some text\n",
    "with open(\"Eisenhower.txt\", 'r') as file:\n",
    "    original_text = file.read()\n",
    "\n",
    "print(original_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for rendering the text to an image and then crop the image to reduce the margins (our function $f$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_text(texttorender, scale=1.0):\n",
    "    lines = texttorender.split('\\n')\n",
    "    img = np.zeros((int(len(lines)*40*scale), int(np.max([len(line) for line in lines])*20*scale)))\n",
    "    for n, textline in enumerate(lines):\n",
    "        img = cv2.putText(img, textline, (10, int((n+1)*40*scale)), cv2.FONT_HERSHEY_COMPLEX,\n",
    "                          scale, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    return img\n",
    "\n",
    "def autocrop(image):\n",
    "    \"\"\"Reducing the size of the image to only include the parts with foreground.\"\"\"\n",
    "    for axis in [0, 1]:\n",
    "        s = np.sum(image>0, axis=axis)\n",
    "        a = 0\n",
    "        b = len(s)-1\n",
    "        while s[a] == 0:\n",
    "            a += 1\n",
    "        a = max(a-5, 0)\n",
    "        while s[b] == 0:\n",
    "            b -= 1\n",
    "        b = min(b+5, len(s)-1)\n",
    "        if axis == 1:\n",
    "            image = image[a:b+1, :]\n",
    "        else:\n",
    "            image = image[:, a:b+1]\n",
    "    return image\n",
    "\n",
    "def f(text_to_render, scale=1.0):\n",
    "    return autocrop(render_text(text_to_render, scale=scale))\n",
    "\n",
    "image_with_text = f(original_text)\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(image_with_text, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the function $g$ (this can take some time, depending on the available computing power)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text = pytesseract.image_to_string(image_with_text)\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the quality\n",
    "\n",
    "A common metric of quality of the OCR is the word error rate, i.e. the number of non-recognised words in relation to the total number of words. This can be done by flexibly matching the original text with the text returned from the OCR.\n",
    "\n",
    "The code shown below uses cython for speed in the Levenshtein calculations. You must have the cython package installed to use this code as it needs to do some compiling on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from levenshtein import wer, cer\n",
    "print(wer.__doc__)\n",
    "print(cer.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(original_text.split()[:20])\n",
    "print(extracted_text.split()[:20])\n",
    "we = wer(original_text, extracted_text)\n",
    "print(\"Word errors:\", we)\n",
    "print(\"WER:\", we/max(len(original_text.split()), len(extracted_text.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(original_text)[:20])\n",
    "print(list(extracted_text)[:20])\n",
    "ce = cer(original_text, extracted_text)\n",
    "print(\"Character errors:\", ce)\n",
    "print(\"CER:\", ce/max(len(list(original_text)), len(list(extracted_text))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A real world example\n",
    "\n",
    "This image is from an old encyclopedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Gutenberg.txt\", 'r', encoding='utf-8') as file:\n",
    "    original_text = file.read()\n",
    "print(original_text)\n",
    "\n",
    "img = cv2.imread(\"Gutenberg.png\", cv2.IMREAD_GRAYSCALE)\n",
    "plt.figure(figsize=(4, 12))\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for extracting the text and calculating the error rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text = pytesseract.image_to_string(img)\n",
    "\n",
    "we = wer(original_text, extracted_text)\n",
    "print(\"Word errors:\", we)\n",
    "print(\"WER:\", we/max(len(original_text.split()), len(extracted_text.split())))\n",
    "\n",
    "ce = cer(original_text, extracted_text)\n",
    "print(\"Character errors:\", ce)\n",
    "print(\"CER:\", ce/max(len(list(original_text)), len(list(extracted_text))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OCR can incorporate knowledge of language in the recognition. Let's try OCR with a language model for Swedish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text = pytesseract.image_to_string(img, lang='swe')\n",
    "\n",
    "we = wer(original_text, extracted_text)\n",
    "print(\"Word errors:\", we)\n",
    "print(\"WER:\", we/max(len(original_text.split()), len(extracted_text.split())))\n",
    "\n",
    "ce = cer(original_text, extracted_text)\n",
    "print(\"Character errors:\", ce)\n",
    "print(\"CER:\", ce/max(len(list(original_text)), len(list(extracted_text))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the error rate for words in greatly improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cropped = img[:1000, :]\n",
    "data = pytesseract.image_to_boxes(img_cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(img_cropped, cmap='gray')\n",
    "for entry in data.split(\"\\n\"):\n",
    "    char, y1, x1, y2, x2, _ = entry.split()\n",
    "    y1 = int(y1)\n",
    "    y2 = int(y2)\n",
    "    x1 = img_cropped.shape[0]-int(x1)\n",
    "    x2 = img_cropped.shape[0]-int(x2)\n",
    "    plt.plot([y1, y2, y2, y1, y1], [x1, x1, x2, x2, x1]) # Plot the box\n",
    "    plt.text(y2, x1, char, color='m') # Plot the OCRed character\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
